\ifx\allfiles\undefined

\documentclass{article}

\begin{document}

\title{Something in Title}

\author{cohomo@blogbus}

\date{}

\maketitle

\fi

\section{Background}

\label{sec:background}

Taihu-Light is a manycore accelerated 100-petaflop supercomputer, with Turtle as its storage filesystem. The Turtle storage system was deploy from 2015 fall to 2016 spring, along with Taihu-Light. It has a high bandwidth and huge capacity to meet the needs of Taihu-Light.
Table1 gives key characteristics of the turtle storage system.

Figure 1 shows a conceptual hardware architecture of how Taihu-Ligth connect to Turtle. Taihu-Light connect its computing nodes to Turtle filesystem using an I/O forwarding network. Every 256 manycore CPUs(4 core SMP) are assigned together as a supernode, which has a full-connect network inside, and connect to I/O forwarding network(a 648-Infiniband switch) with 2 FDR links.

On the other side of the I/O forwarding network, 240 server nodes are connected via one link per node. These nodes are connected to the storage network(also a 648 Infiniband switch) using another link. They transfer the I/O requests from Taihu-Light computing nodes to the Turtle filesystem.

At the backend, Turtle use a NetApp type-num? RAID controller as its building block. Each controller has 6 RAID6(8+2) disk enclosure containing a total of 60 1.2TB SAS disk drives. Two OSS hosts are connect to the controller via 2 fiber channel on each host for path redundancy and each OSS has 3 OSTs (when the pair node gives up resources and redundancy is activate, there will be 6 OSTs) to store data. In total, there are 144 building blocks. Together with 4 MDS, they are built into two file system namespaces(online1 and online2) using the storage network. The resulting system has a capacity about 7.6PB(10PB in raw)£¬with 2 file system about 3.8PB each(after enclosure and format).

Figure 2 shows the I/O path and the software architecutre.
Taihu-Light use Lwfs[reference] as I/O forwarding software to access Turtle. Lwfs is a fuse based, network file system. It uses a C/S structure something like nfs. The computing node uses fuse to port I/O request from VFS to a forwarding client, and deliver it to the server side through network. The forwarding server receive the request and transfer it to a client of Lustre, which served as the backend file system. 
Lustre handled the I/O request and return reply to the forwarding server. The forwarding server give the reply back to the client, and client wake up the application who is sleeping on fuse, so as to finish the whole I/O process.

Each forwarding node has a bandwidth more than 1GB/s read/write, so the forwarding system has more than 240GB/s transfer rate. And at the backend each NetAPP controller gives about 1.8GB/s(theoritically 2.0GB/s) bandwidth, which means the backend file system has about 260GB/s raw bandwidth. As a whole, Turtle is tested to deliver an aggregate 240GB/s for reads and 220GB/s for writes which translates into 200+GB/s aggregate read and write performance at the file system level.

As well as Taihu-Light supercomputer, Turtle servers some other analysis and visualization platforms. Various applications are run on Turtle, so there is a challenge to understand the application's I/O behavior as well as Turtle storage system itself.

Through the deployments of Turtle, we have developed a comprehensive monitoring tools. Through it, we are able to get the I/O trace at different level of the I/O system. Using the trace, we find a lot of interesting details of the system as well as some of the applications. 

......

......

lalal


By considering the design of the Taihu-Light and Turtle storage system, we identified three major points to collect the trace, illustrated in Figure 1

1) I/O operations are always raised by the applications, So the syscall traces are collected on the computing nodes to show the characteristics and patterns of the applications.  

2) On the I/O forwarding node, aggregated Lustre client's RPCs are recorded to show the network flow. Requests from the computing nodes will be aggregated  

3) On the I/O server, aggregated Lustre OSTs operations are recorded to show the disk access. Requests from the 



It is not necessary to record the detailed trace on I/O forwarding node and I/O server. Requests submitted from the computing nodes will be aggregated by the Lustre client on the I/O forwarding node, and the aggregated requests will be remapped and submitted to   

These points contain the main statstics through the I/O path and give an overview of the I/O process on the system level. 
 

\ifx\allfiles\undefined

\end{document}

\fi
